diff --git a/include/mimalloc-atomic.h b/include/mimalloc-atomic.h
index 2d725a2..3944f26 100644
--- a/include/mimalloc-atomic.h
+++ b/include/mimalloc-atomic.h
@@ -11,9 +11,9 @@ terms of the MIT license. A copy of the license can be found in the file
 // --------------------------------------------------------------------------------------------
 // Atomics
 // We need to be portable between C, C++, and MSVC.
-// We base the primitives on the C/C++ atomics and create a mimimal wrapper for MSVC in C compilation mode. 
-// This is why we try to use only `uintptr_t` and `<type>*` as atomic types. 
-// To gain better insight in the range of used atomics, we use explicitly named memory order operations 
+// We base the primitives on the C/C++ atomics and create a mimimal wrapper for MSVC in C compilation mode.
+// This is why we try to use only `uintptr_t` and `<type>*` as atomic types.
+// To gain better insight in the range of used atomics, we use explicitly named memory order operations
 // instead of passing the memory order as a parameter.
 // -----------------------------------------------------------------------------------------------
 
@@ -269,7 +269,7 @@ static inline intptr_t mi_atomic_subi(_Atomic(intptr_t)*p, intptr_t sub) {
   return (intptr_t)mi_atomic_addi(p, -sub);
 }
 
-// Yield 
+// Yield
 #if defined(__cplusplus)
 #include <thread>
 static inline void mi_atomic_yield(void) {
@@ -291,7 +291,11 @@ static inline void mi_atomic_yield(void) {
        defined(__aarch64__) || defined(__powerpc__) || defined(__ppc__) || defined(__PPC__))
 #if defined(__x86_64__) || defined(__i386__)
 static inline void mi_atomic_yield(void) {
+#if defined(__llir__)
+  __asm__ volatile ("x86_pause" ::: "memory");
+#else
   __asm__ volatile ("pause" ::: "memory");
+#endif
 }
 #elif defined(__aarch64__)
 static inline void mi_atomic_yield(void) {
diff --git a/include/mimalloc-internal.h b/include/mimalloc-internal.h
index 06d31db..2f3e61b 100644
--- a/include/mimalloc-internal.h
+++ b/include/mimalloc-internal.h
@@ -306,13 +306,13 @@ mi_heap_t*  _mi_heap_main_get(void);    // statically allocated main backing hea
 
 #if defined(MI_MALLOC_OVERRIDE)
 #if defined(__MACH__) // OSX
-#define MI_TLS_SLOT               89  // seems unused? 
+#define MI_TLS_SLOT               89  // seems unused?
 // other possible unused ones are 9, 29, __PTK_FRAMEWORK_JAVASCRIPTCORE_KEY4 (94), __PTK_FRAMEWORK_GC_KEY9 (112) and __PTK_FRAMEWORK_OLDGC_KEY9 (89)
 // see <https://github.com/rweichler/substrate/blob/master/include/pthread_machdep.h>
 #elif defined(__OpenBSD__)
-// use end bytes of a name; goes wrong if anyone uses names > 23 characters (ptrhread specifies 16) 
+// use end bytes of a name; goes wrong if anyone uses names > 23 characters (ptrhread specifies 16)
 // see <https://github.com/openbsd/src/blob/master/lib/libc/include/thread_private.h#L371>
-#define MI_TLS_PTHREAD_SLOT_OFS   (6*sizeof(int) + 4*sizeof(void*) + 24)  
+#define MI_TLS_PTHREAD_SLOT_OFS   (6*sizeof(int) + 4*sizeof(void*) + 24)
 #elif defined(__DragonFly__)
 #warning "mimalloc is not working correctly on DragonFly yet."
 //#define MI_TLS_PTHREAD_SLOT_OFS   (4 + 1*sizeof(void*))  // offset `uniqueid` (also used by gdb?) <https://github.com/DragonFlyBSD/DragonFlyBSD/blob/master/lib/libthread_xu/thread/thr_private.h#L458>
@@ -411,7 +411,7 @@ static inline mi_slice_t* mi_page_to_slice(mi_page_t* p) {
 
 // Segment belonging to a page
 static inline mi_segment_t* _mi_page_segment(const mi_page_t* page) {
-  mi_segment_t* segment = _mi_ptr_segment(page); 
+  mi_segment_t* segment = _mi_ptr_segment(page);
   mi_assert_internal(segment == NULL || ((mi_slice_t*)page >= segment->slices && (mi_slice_t*)page < segment->slices + segment->slice_entries));
   return segment;
 }
@@ -815,6 +815,23 @@ static inline uintptr_t _mi_thread_id(void) mi_attr_noexcept {
 
 // TLS register on x86 is in the FS or GS register, see: https://akkadia.org/drepper/tls.pdf
 static inline void* mi_tls_slot(size_t slot) mi_attr_noexcept {
+#if defined(__llir__)
+#if defined(__i386__)
+  #error "not implemented"
+#elif defined(__MACH__) && defined(__x86_64__)
+  #error "not implemented"
+#elif defined(__x86_64__) && (MI_INTPTR_SIZE==4)
+  #error "not implemented"
+#elif defined(__x86_64__)
+  void** tcb;
+  __asm__("get.i64  %0, $fs\n" : "=r"(tcb) : :);
+  return tcb[slot];
+#elif defined(__arm__)
+  #error "not implemented"
+#elif defined(__aarch64__)
+  #error "not implemented"
+#endif
+#else
   void* res;
   const size_t ofs = (slot*sizeof(void*));
 #if defined(__i386__)
@@ -839,10 +856,28 @@ static inline void* mi_tls_slot(size_t slot) mi_attr_noexcept {
   res = tcb[slot];
 #endif
   return res;
+#endif
 }
 
 // setting is only used on macOSX for now
 static inline void mi_tls_slot_set(size_t slot, void* value) mi_attr_noexcept {
+#if defined(__llir__)
+#if defined(__i386__)
+  #error "not implemented"
+#elif defined(__MACH__) && defined(__x86_64__)
+  #error "not implemented"
+#elif defined(__x86_64__) && (MI_INTPTR_SIZE==4)
+  #error "not implemented"
+#elif defined(__x86_64__)
+  void** tcb;
+  __asm__("get.i64  %0, $fs\n" : "=r"(tcb) : :);
+  tcb[slot] = value;
+#elif defined(__arm__)
+  #error "not implemented"
+#elif defined(__aarch64__)
+  #error "not implemented"
+#endif
+#else
   const size_t ofs = (slot*sizeof(void*));
 #if defined(__i386__)
   __asm__("movl %1,%%gs:%0" : "=m" (*((void**)ofs)) : "rn" (value) : );  // 32-bit always uses GS
@@ -865,6 +900,7 @@ static inline void mi_tls_slot_set(size_t slot, void* value) mi_attr_noexcept {
 #endif
   tcb[slot] = value;
 #endif
+#endif
 }
 
 static inline uintptr_t _mi_thread_id(void) mi_attr_noexcept {
@@ -903,7 +939,7 @@ static inline size_t mi_ctz(uintptr_t x) {
 #endif
 }
 
-#elif defined(_MSC_VER) 
+#elif defined(_MSC_VER)
 
 #include <limits.h>       // LONG_MAX
 #define MI_HAVE_FAST_BITSCAN
@@ -914,7 +950,7 @@ static inline size_t mi_clz(uintptr_t x) {
   _BitScanReverse(&idx, x);
 #else
   _BitScanReverse64(&idx, x);
-#endif  
+#endif
   return ((MI_INTPTR_BITS - 1) - idx);
 }
 static inline size_t mi_ctz(uintptr_t x) {
@@ -924,7 +960,7 @@ static inline size_t mi_ctz(uintptr_t x) {
   _BitScanForward(&idx, x);
 #else
   _BitScanForward64(&idx, x);
-#endif  
+#endif
   return idx;
 }
 
@@ -954,7 +990,7 @@ static inline size_t mi_clz32(uint32_t x) {
 }
 
 static inline size_t mi_clz(uintptr_t x) {
-  if (x==0) return MI_INTPTR_BITS;  
+  if (x==0) return MI_INTPTR_BITS;
 #if (MI_INTPTR_BITS <= 32)
   return mi_clz32((uint32_t)x);
 #else
@@ -985,9 +1021,9 @@ static inline size_t mi_bsr(uintptr_t x) {
 // ---------------------------------------------------------------------------------
 // Provide our own `_mi_memcpy` for potential performance optimizations.
 //
-// For now, only on Windows with msvc/clang-cl we optimize to `rep movsb` if 
-// we happen to run on x86/x64 cpu's that have "fast short rep movsb" (FSRM) support 
-// (AMD Zen3+ (~2020) or Intel Ice Lake+ (~2017). See also issue #201 and pr #253. 
+// For now, only on Windows with msvc/clang-cl we optimize to `rep movsb` if
+// we happen to run on x86/x64 cpu's that have "fast short rep movsb" (FSRM) support
+// (AMD Zen3+ (~2020) or Intel Ice Lake+ (~2017). See also issue #201 and pr #253.
 // ---------------------------------------------------------------------------------
 
 #if defined(_WIN32) && (defined(_M_IX86) || defined(_M_X64))
@@ -1011,7 +1047,7 @@ static inline void _mi_memcpy(void* dst, const void* src, size_t n) {
 
 
 // -------------------------------------------------------------------------------
-// The `_mi_memcpy_aligned` can be used if the pointers are machine-word aligned 
+// The `_mi_memcpy_aligned` can be used if the pointers are machine-word aligned
 // This is used for example in `mi_realloc`.
 // -------------------------------------------------------------------------------
 
